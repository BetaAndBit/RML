var textForPages =[" ","The Hitchhiker\u2019s Guide to Responsible Machine Learning The R version\u000AAuthors:\u000APrzemys\u0142aw Biecek, Anna Kozak, Aleksander Zawada\u000AIllustrations and cover:\u000AAleksander Zawada\u000AReviewer:\u000A\u0141ukasz Rajkowski, PhD\u000AWebpage:\u000Ahttps://betabit.wiki/RML\u000APublisher:\u000APublishing House of the Warsaw University of Technology ul. Polna 50, 00\u2013644 Warszawa, tel. +48 22 234 70 83\u000Atel. +48 22 234 75 03, e-mail: oficyna@pw.edu.pl\u000AISBN:\u000A978-83-8156-264-5 Printing & binding:\u000AJKB Print\u000AEdition I Warsaw 2021\u000A","All right, but how do you build predictive models in a responsible way?\u000AThis is a question I am often asked by data scientists at different levels of experience. Seemingly simple but at the same time challen- ging because there are several orthogonal threads and perspectives of different stakeholders that shall be addressed.\u000AModel developers focus on automation of model training, moni- toring of model performance, debugging, and other MLOps related matters. Users of predictive models are more interested in expla- inability, transparency and security, while fairness, bias, ethics are issues of interest to society.\u000AIn this book, when showing topics related to Responsible Machine Learning (RML), we focus on three essential elements.\u000AAlgorithms - Often, to capture complex relationships in data, you need to use advanced and elastic machine learning algorithms. The- se, however, should not be used without understanding how they work. So a discussion about responsible modelling must touch on the topic of how complex models work.\u000ASoftware - Training of advanced models is a computationally de- manding process. The libraries that allow for efficient training are low-level engineering masterpieces. Professionals use good tools, so a story about responsible modelling must include a section related to good software.\u000AProcess - Predictive modelling is not only about tools but also about planning, logistic, communication, deadlines and objectives. The process of data and model exploration is iterative, as in each iteration, we head towards better and better models. Knowing the tools does not help much if you do not know when and how to use them. Therefore, to talk about responsible modelling, we need to talk about the processes behind modelling.\u000AThis book is a unique entanglement of all these aspects together at the same time. You will find here selected modern machine learning techniques and the intuition behind them. Methods are supplemen- ted by code snippets with examples in R language1. The process is shown through a comic book describing the adventures of two characters, Beta and Bit. The interaction of these two shows the de- cisions that analysts often face, whether to try a different model, try another technique for exploration or look for another data \u2014 qu- estions like: how to compare models or how to validate them.\u000AModel development is responsible and challenging work but al- so an exciting adventure. Sometimes textbooks focus only on the technical side, losing all the fun. Here we are going to have it all.\u000APrzemys\u0142aw Biecek Warszawa, 2021\u000A1 R Core Team. R: A Language and Environment for Statistical Com- puting. R Foundation for Sta- tistical Computing, Vienna, Au- stria, 2021. URL https://www. R-project.org/\u000ABut what is it all about?\u000A"," "," ","Prelude\u000A6\u000A2 We use an example actually built on real data to predict the risk of severe Covid disease progression. But the approach presented can be applied to a very broad class of pro- blems.\u000APredictive models have been used throughout entire human hi- story. Priests in ancient Egypt were predicting when the flood of the Nile or a solar eclipse would come. Developments in statistics, increasing availability of datasets, and increasing computing power allow predictive models to be built faster and deployed in a rapidly growing number of applications.\u000AToday, predictive models are used virtually everywhere. The plan- ning of the supply chain for a large corporation, recommending lunch or a movie for the evening, or predicting traffic jams in a city. Newspapers are full of exciting applications.\u000ABut how are such predictive models developed?\u000AIn the following pages, we go through a life cycle of an example predictive model2 from the concept phase, through design, training, checking, to the deployment. We present an agile approach to bu- ilding and exploring Machine Learning (ML) models, inspired by the agile approach to software development. The main principles of Agile ML are: a continuous adaptation to newly acquired knowled- ge, continuous prototyping of the solution, dynamic planning, and effective communication. The life cycle of a predictive model is sum- marized by the diagram below.\u000AAgile manifesto\u000A                         https:\u000AAgile_software_development\u000AFigure 1: Developing a predictive model often involves many itera- tions. In this book too, iteration by iteration, we build increasin- gly complex models, compare them against other, and extract useful in- formation through various Expla- natory Model Analysis (EMA) tech- niques.\u000ASubsequent iterations consist of exploration of literature, data and models, assembly of new solutions and validation after validation. But in addition to these steps, we also show the concept phase, in which the problem to be solved is speci- fied, and the deployment phase.\u000A//en.wikipedia.org/wiki/\u000A Conception\u000ADeliver\u000AAssembly\u000AExplore\u000A    Evaluate\u000A3 Gareth James, Daniela Witten, Tre- vor Hastie, and Robert Tibshira- ni. An Introduction to Statistical Learning: with Applications in R. Springer, 2013. URL https://www. statlearning.com/\u000AWe present the life cycle of model development and validation on an example of a binary classification model. We start with a sim- ple model derived from domain knowledge and expand it up to a fully data-driven random forest model with automatically tuned hyperparameters. The description of the methods is supplemented with code snippets that you can use to replicate all presented results yourself. It\u2019s worth playing around with these codes to understand better how described methods work.\u000ADue to the limited space, the descriptions of the methods of both machine learning algorithms and explainable artificial intelligence are brief. If you want to learn more about predictive modelling, I highly recommend the book An Introduction to Statistical Learning (ISL)3. For those interested in a more detailed description of Explana- tory Model Analysis (EMA) and eXplainable Artificial Intelligence (XAI), you will find much more in the book Explanatory Model Ana-\u000A","7\u000Alysis4. Both are available in paperback but also can be read free of\u000Acharge in electronic form.\u000AThe modelling approach presented in this book is inspired by the\u000Apaper Statistical modeling: the two cultures by Leo Breiman5. It pre- sents two views of modelling, one focused on building models that describe the laws of nature and the other describing models focused on the effectiveness of predicting a certain trait. As we will show in this book, a bridge can be built between these approaches. Effective models can and should be used to extract knowledge about a do- main, and such knowledge can be furthered transformed into even more effective models.\u000APrelude\u000A4 Przemyslaw Biecek and Tomasz Burzykowski. Explanatory Model Analysis. Chapman and Hall/CRC, New York, 2021. URL https:// pbiecek.github.io/ema/\u000A5 Leo Breiman. Statistical modeling: the two cultures. Statistical Science, 16(3):199\u2013231, 2001b\u000AFigure 2: The first part of this bo- ok is devoted to the transforma- tion of knowledge and data into a model and then into predictions. The second part of this book discus- ses how to learn from predictions how the model works and how to extract information about the do- main from the predictive model.\u000A   Knowledge\u000A+ Model Prediction\u000AData\u000A  Another interesting point made by Leo Breiman in the article men- tioned above is the Rashomon perspective for predictive modelling, i.e. situation in which several equally good models describe the sa- me phenomenon differently. In this book, we show how to examine what different models say about the data. We introduce a pyramid for model exploration that forms a language in which we can show and cross-compare stories learned by different predictive models.\u000ASARS-COV-2 case study\u000ATo demonstrate what responsible predictive modelling looks like, we used data obtained from the collaboration with the Polish In- stitute of Hygiene in modelling mortality after Covid infection. We realize that data on Coronavirus disease can evoke negative feelings. However, it is a good example of how predictive modelling can di- rectly impact our society and how data analysis allows us to deal with complex, important and topical problems.\u000AAll the results presented in this book can be independently repro- duced using the snippets and instructions presented in this book. If you do not want to retype them, then all the examples, data, and codes can be found on the webpage of this book6. Please note that the data presented at this URL is artificially generated to mirror re- lations in actual data. But it does not contain real patients data.\u000AThe procedure outlined here is presented for mortality modelling, but the same process can be replicated whether modelling patient survival, housing pricing, or credit scoring.\u000A6 https://betabit.wiki/RML\u000A"," "," ","Model Assembly\u000A10\u000AAs you will see in a minute, we can create a model without new data.\u000AHello model!\u000AWhen browsing through examples of predictive modelling, one may get the wrong impression that the life cycle of the model begins with the data from the internet and ends with validation on an indepen- dent dataset. However, this is an oversimplification.\u000AThe life cycle of a predictive model begins with a well-defined problem. In this example, we are looking for a model that assesses the risk of death after diagnosed Covid. We don\u2019t want to guess who will survive and who won\u2019t. Instead, we want to construct a score that allows us to sort patients by their individual risk. Why do we need such a model? For example, those at higher risk of death co- uld be given higher protection, such as providing them with pulse oximeters or preferentially vaccinating them. For this reason, in the following sections, we introduce and use model performance me- asures that evaluate rankings of scores, such as Area Under Curve (AUC). Pick a model evaluation measure suitable for the problem posed.\u000AHaving defined the problem we want to solve, we can move to the next step, which is to collect all the available information. Often the solution to the problem can be found in the literature, whether in the form of a ready-made feature prediction function, a discussion of what features are important, or sample data.\u000AIf there are no ready-to-use solutions and we have to collect the data ourselves, it is always worth considering where and what data to collect in order to build the model on a representative sample7. The problem of data representativeness is a topic for a separate book. Incorrectly collected data will create biases that are hard to discover and even harder to fix.\u000AWe think of a predictive model as a function that computes a cer- tain predictions for specific input data. Usually, such a function is built automatically based on the data. But technically, the model can be any function defined in any way.\u000AOur first model will be based on statistics collected by the Centers for Disease Control and Prevention (CDC)8. That\u2019s right; sometimes, we don\u2019t need raw data to build a predictive model. We\u2019ll start by turning a table with mortality statistics into a model.\u000A7 In our study, we used data on all patients reached by the sanitary in- spectorate between Match and Au- gust 2020. It would seem that data collected in this way would be free of biases, but we were able to detect at least one. In April, the pandemic spread faster among coal mine wor- kers, who were more likely to be young men, which may have influ- enced the fluctuations in mortality.\u000A8 https://www.cdc.gov/\u000AFigure 3: Mortality statistics as pre- sented at CDC website https:// tinyurl.com/CDCmortality acces- sed on May 2021. This table shows rate ratios compared to the group 18- to 29-year-olds (selected as the reference group because it has ac- counted for the largest cumulative number of COVID-19 cases compa- red to other age groups).\u000A ","11\u000AModel Assembly\u000AR snippets\u000AA predictive model is a function that transforms n \u21E5 p data frame with p variables for n observations into a vector of n predictions. For further examples, below, we define a function that calculates odds of Covid related death based on statistics from the CDC website for different age groups9.\u000A__\u000Acdc risk <- function(x, base risk = 0.00003) {\u000A  rratio <- rep(7900, nrow(x))\u000A  rratio[which(x$Age < 84.5)] <- 2800\u000A  rratio[which(x$Age < 74.5)] <- 1100\u000A  rratio[which(x$Age < 64.5)] <- 400\u000A  rratio[which(x$Age < 49.5)] <- 130\u000A  rratio[which(x$Age < 39.5)] <- 45\u000A  rratio[which(x$Age < 29.5)] <- 15\u000A  rratio[which(x$Age < 17.5)] <- 1\u000A  rratio[which(x$Age < 4.5)]  <- 2\u000A  rratio * base_risk\u000A}\u000Asteve <- data.frame(Age = 25, Diabetes = \u0022Yes\u0022)\u000Acdc_risk(steve)\u000A## [1] 0.00045\u000APredictive models may have different structures. To work respon- sibly with a large number of models, a uniform standardized inter- face is needed. In this book, we use the abstraction implemented in the DALEX package10.\u000AThe explain function from this package creates an explainer11, i.e. a wrapper for the model that will allow you to work uniformly with objects of very different structures. The first argument is a model. It can be an object of any class. The second argument is a function that calculates the vector of predictions. DALEX package can often guess which function is needed for a specific model, but in this book, we explicitly show it to understand better how the wrapper works. The type argument specifies the model type and the label specifies an unique name that appear in the plots.\u000Alibrary(\u0022DALEX\u0022)\u000Amodel_cdc <- DALEX::explain(cdc_risk,\u000Apredict_function = function(m, x) m(x), type = \u0022classification\u0022,\u000Alabel = \u0022CDC\u0022)\u000Apredict(model_cdc, steve)\u000A## [1] 0.00045\u000AUsing the explain function may seem like an unnecessary com- plication at the moment, but on the following pages, we show how it simplifies the work.\u000AThe biggest advantage of such a constructed object (explainer) is its standardized structure, to some degree independent from the internal structure of the model.\u000A9 There was no risk for the referen- ce group in the table 3. It is not relevant if we are only interested in the ranking of relative risks. But to make the predictions easier to in- terpret we use here the relative risk determined on Polish data, which is 0.003% for the reference group.\u000A10 Przemyslaw Biecek.\u000AExplainers for Complex Predicti- ve Models in R. Journal of Ma- chine Learning Research, 19(84):1\u2013 5, 2018. URL https://jmlr.org/ papers/v19/18-416.html\u000A11 Explainer is an object/adapter that wraps a model and creates a uniform structure and interface for operations.\u000ADALEX:\u000A"," "," ","Data Preparation and Understanding 14 Exploratory Data Analysis (EDA)\u000A12 Please note that the attached data are not the real data collected for epidemiological purposes, but arti- ficially generated data preserving the structure and relationships in the actual data.\u000ATo build a model, we need good data. In Machine Learning, the word good means a large amount of representative data. Unfortuna- tely, collecting representative data is not easy nor cheap and often requires designing and conducting a specific experiment.\u000AThe best possible scenario is that one can design and run a study to collect the necessary data. In less comfortable situations, we look for \u0022natural experiments,\u0022 i.e., data that have been collected for ano- ther purpose but that can be used to build a model. Here we will use the data12 collected through epidemiological interviews. There will be a lot of data points, and it should be rather representative, although unfortunately, it only involves symptomatic patients who are tested positive for SARS-COV-2.\u000AThe data is divided into two sets covid_spring and covid_summer. The first is acquired in spring 2020 and will be used as training data, while the second dataset is acquired in the summer and will be used for validation. In machine learning, model validation is performed on a separate data set called validation data. This controls the risk of overfitting an elastic model to the training data. If we do not have a separate set, then it is generated using cross-validation, out- of-sample, out-of-time or similar internal data splitting techniques.\u000AR snippets\u000AThe R software offers hundreds of specialized solutions for explora- tory data analysis. Certainly, many valuable solutions can be found in the book \u201ER for Data Science\u201D13, but there is much more. Below we show just three examples. Let\u2019s start with loading the data.\u000Acovid_spring <- read.table(\u0022covid_spring.csv\u0022, sep =\u0022;\u0022, header = TRUE)\u000Acovid_summer <- read.table(\u0022covid_summer.csv\u0022, sep =\u0022;\u0022, header = TRUE)\u000AWe use the package ggplot2 to draw a simple histogram for Age, and ggmosaic to draw a mosaic plot for Diabetes. Note that the plots in the margins are graphically edited, so they look slightly different from the plots generated by these short instructions.\u000A# See Figure 4\u000Alibrary(\u0022ggplot2\u0022) ggplot(covid_spring) +\u000A     geom_histogram(aes(Age, fill = Death))\u000A# See Figure 5\u000Alibrary(\u0022ggmosaic\u0022) ggplot(data = covid_spring) +\u000Ageom_mosaic(aes(x=product(Diabetes), fill = Death))\u000A13 Hadley Wickham and Garrett Grolemund. R for Data Science: Im- port, Tidy, Transform, Visualize, and Model Data. O\u2019Reilly Media, Inc., 2017\u000A  750 500 250 0\u000A       0 25 50 75 100 Age\u000AFigure 4: Histogram for the Age variable by survivor status.\u000A Yes\u000ANo\u000A   Figure 5: The mosaic plot shows that there are significantly fewer people with diabetes, but among them, the mortality is higher.\u000ANo Yes Diabetes\u000ADeath\u000ACount\u000A","15 Data Preparation and Understanding\u000AA handy way to summarise tabular data for groups is the so- called \u201ETable 1\u201D. This is a summary of the main characteristics of each variable broken down into groups defined by the variable of interest (here, binary information about Covid death). The name stems from the fact that this summary of the data is usually the first table to be shown in many epidemiological and related scientific journals.\u000Alibrary(\u0022tableone\u0022)\u000ACreateTableOne(vars = colnames(covid_spring)[1:10],\u000A                         data = covid_spring,\u000A                         strata = \u0022Death\u0022)\u000A# Stratified by Death # NoYes\u000A#n\u000A# Gender = Male (%)\u000A# Age (mean (SD))\u000A# CardiovascularDiseases = Yes (%) # Diabetes = Yes (%)\u000A# Neurological.Diseases = Yes (%) # Kidney.Diseases = Yes (%)\u000A# Cancer = Yes (%)\u000A# Hospitalization = Yes (%)\u000A# Fever = Yes (%)\u000A# Cough = Yes (%)\u000A9487           513\u000A4554 (48.0)    271 (52.8) 0.037\u000AOne of the most important rules to remember when building a predictive model is: Do not condition on future! Note, that in the discussed case variables Hospitalization, Fever or Cough are not good predictors because they are not known in advance before infection. So they are not useful in our case.\u000AIn the following lines, we remove invalid variables from both data sets.\u000Aselected_vars <- c(\u0022Gender\u0022, \u0022Age\u0022, \u0022Cardiovascular.Diseases\u0022, \u0022Diabetes\u0022, \u0022Neurological.Diseases\u0022, \u0022Kidney.Diseases\u0022, \u0022Cancer\u0022, \u0022Death\u0022)\u000A# use only selected variables\u000Acovid_spring <- covid_spring[,selected_vars]\u000Acovid_summer <- covid_summer[,selected_vars]\u000AData exploration and cleaning often consume most of the time spent on data analysis. Here we have only touched on exploration, but even after this initial analysis helped determine that Age is an important characteristic (we will confirm this later). From the list of variables, we removed those that are unknown before the disease develops (like hospitalization status). We build further models only on variables from the selected_vars vector.\u000A44.19 (18.32) 74.44\u000A(13.2) <0.001\u000A 839 ( 8.8)\u000A 260 ( 2.7)\u000A 127 ( 1.3)\u000A 111 ( 1.2)\u000A 158 ( 1.7)\u000A2344 (24.7)\u000A3314 (34.9)\u000A3062 (32.3)\u000A273 (53.2)\u000A 78 (15.2)\u000A 57 (11.1)\u000A 62 (12.1)\u000A 68 (13.3)\u000A481 (93.8)\u000A335 (65.3)\u000A253 (49.3)\u000A<0.001\u000A<0.001\u000A<0.001\u000A<0.001\u000A<0.001\u000A<0.001\u000A<0.001\u000A<0.001\u000A"," "," ","Model Audit\u000A18\u000A14Iff:Rp !Risafunctionthat\u000Apredicts the value of yi using obse-\u000AModel Performance\u000ADepending on the type of predictive problem and what we assume about the distribution of the outcome, various measures of model performance can be used. Here is a short summary; find a more detailed description in the EMA book.\u000AFor regression problems, when we predict a quantitative varia- ble, especially when we assume Gaussian noise, commonly used performance measures are Mean Squared Error14 and Rooted Mean Squared Error15.\u000AFor binary classification problems, the outcome is commonly sum- marized with a 2 \u21E5 2 contingency table with possible results coded as True Positive, True Negative, False Positive, and False Negative. Positive means that the test suggests a pregnancy, while Negative means no pregnancy. True and False describe whether the test result is correct or not. Below is an example of such a table for a simple\u000A\u201Emorning sickness\u201D test for the pregnancy.\u000Arvationx thePn i1n2\u000AMSE = np i (f(xi) - yi) 15 RMSE = MSE\u000ATabela 1: Is morning sickness a good pregnancy test? This table is based on GetTheDiagnosis data http://getthediagnosis.org/ diagnosis/Pregnancy.htm. For example, FN = 61 means that out of 100 pregnant women for 61 the test suggested otherwise. Exempla- ry measures of performance are shown in the last row and column.\u000A16 ACC = (T P + T N)/n\u000A17 Sens = T P/(T P + FN)\u000A18 Spec = TN/(TN + FP)\u000A19 Prec = T P/(T P + FP)\u000A20 Recall = TP/(TP + FN) 21 F1 = 2 Prec\u21E4Recall\u000AMorning sickness\u000A/ pregnancy Has sickness Has not\u000APregnant Not Pregnant\u000ATP=39 FP=150 PPV=Prec=20.6% FN=61 TN=850 NPV=93.3%\u000A   Prec+Recall 22PPV= TP\u000ABased on such contingency table, the most commonly used measu- res of performance are Accuracy16, Sensitivity17, Specificity18, Preci- sion19 , Recall20 , F1 score21 , Positive Predicted Value22 and Negative Predicted Value23 .\u000ANote that, in the covid-mortality-risk-assessment problem, we are not interested in the binary prediction survived/dead, but rather in the validity of the ranking of risk scores. For such types of problems, instead of a contingency table, one looks at Receiver Operating Cha- racteristic (ROC) curve, and the commonly used measure of perfor- mance is the Area Under the ROC curve (AUC). Figure 6 shows how this measure is constructed.\u000ATP+FP 23NPV= TN\u000AT N+FN\u000AFigure 6: Panel A shows the distri- bution of scores obtained from the CDC model for the test data divi- ded by the survival status. By ta- king different cutoffs, one can turn such numerical scores into binary decisions. For each such split, the Sensitivity and 1-Specificity can be calculated and drawn on a plot. The CDC model returns only nine different values, making it reasona- ble to consider ten different cutoffs.\u000APanel B shows these 10 points corresponding to different cutoffs. The ROC curve is the piecewise li- ne connecting these points, and the AUC is the area under this curve. The AUC takes values from 0 to 1, where 1 is the perfect ranking and a purely random ranking leads to an AUC of 0.5.\u000AA) Distribution of scores\u000AB) Receiver Operator Characteristic\u000ASensitivity = Recall = 39%\u000ASpecificity = 85%\u000AF1 = 33.8%\u000A Among those who died 600\u000A40 20\u000AAmong those who survived 2000\u000A1500 1000 500\u000A        3e-05 6e-05 0.00045 0.00135 0.0039 0.012 0.033 0.084 0.2370 scores\u000A 1.00\u000A0.75\u000A0.50\u000A0.25\u000A0.00\u000A0.00 0.25\u000A0.50 0.75 1.0 1 - Specificity\u000A                        0\u000ANumber of patients\u000ASensitivity\u000A","19\u000AModel Audit\u000AR snippets\u000AThere are many measures for evaluating predictive models, and they are located in various R packages (i.e. ROCR, measures, mlr3measures). For simplicity, in this example, we show only model performance measures implemented in the DALEX package.\u000AFirst, we need an explainer with specified validation data (here covid_summer) and the corresponding response variable.\u000Amodel_cdc <- DALEX::explain(cdc_risk,\u000Apredict_function = function(m, x) m(x),\u000A                   data  = covid_summer,\u000A                   y     = covid_summer$Death == \u0022Yes\u0022,\u000A                   type  = \u0022classification\u0022,\u000A                   label = \u0022CDC\u0022)\u000AModel exploration starts with an assessment of how good is the model. The DALEX::model_performance function calculates a set of measures for a specified type of task, here classification.\u000Amp_cdc <- model_performance(model_cdc, cutoff = 0.1) mp_cdc\u000A# Measures for:  classification\u000A# recall     : 0.2188841\u000A# precision  : 0.2602041\u000A# f1\u000A# accuracy\u000A# auc\u000A#\u000A# Residuals:\u000A# 0% 10% 20% 30% 40% 50% # -0.23700 -0.03300 -0.01200 -0.01200 -0.00390 -0.00390 # 60% 70% 80% 90% 100%\u000A# -0.00135 -0.00135 -0.00045 -0.00006 0.99955\u000ANote: The model is evaluated on the data given in the explainer. Use DALEX::update_data() to specify another dataset, e.g. traiing data covid_spring.\u000Amodel_cdc <-  update_data(model_cdc,\u000A                   data  = covid_spring,\u000A                   y     = covid_spring == \u0022Yes\u0022)\u000ANote: Explainer knows whether the model is for classification or re- gression, so it automatically selects the right measures. It can be overridden if needed.\u000AThe S3 generic plot function draws a graphical summary of the model performance. With the geom argument, one can determine the type of chart.\u000A# ROC curve, see Figure 6\u000Aplot(mp_cdc, geom = \u0022roc\u0022)\u000A# LIFT curve, see Figure 7\u000A       _\u000Aplot(mp cdc, geom = \u0022lift\u0022)\u000A: 0.2377622\u000A: 0.9673\u000A: 0.906654\u000A Lift chart\u000AModel CDC\u000A 40 30 20 10\u000A0.00 0.25\u000A0.50 0.75 1.00 Positive rate\u000A         Figure 7: LIFT curve, one of the many graphical statistics used in summarizing the quality of scores, often used in credit risk. The OX axis presents the fraction of assi- gned credits, and the OY axis pre- sents the ratio of the Sensitivity of the tested model to the Sensitivity of the random model.\u000ALift\u000A"," "," ","Model Assembly\u000A22\u000A24 L. Breiman, J. H. Friedman, R. A. Olshen, and C. J. Stone. Classifi- cation and Regression Trees. Wad- sworth and Brooks, Monterey, CA, 1984\u000AGrow a tree\u000AThere are hundreds of different methods for training machine le- arning models available to experienced data scientists. One of the oldest and most popular are tree-based algorithms, first introduced in the book Classification And Regression Trees24 and commonly called CART. The general idea for this class of algorithms may be described as follows.\u000A1. Start with a single node (root) with a full dataset.\u000A2. For a current node, find a candidate split for the data in this node. To do this, consider every possible variable, and for each variable, consider every possible cutoff (for continuous variable) or a sub-\u000Aset of levels (for categorical variable). Select split that maximizes\u000Aselected measure of separation (see below).\u000A3. Check a stopping criteria like the minimum gain in node purity\u000Aor depth of a tree. If the stopping criteria are met, then (obviously) stop. Otherwise, partition the current node into two child nodes and go to step 2 for each child node separately.\u000AThere are two crucial choices here. First is the measure of separa- tion. We illustrate it by considering splits of the Age variable for our dataset. For practical reasons, let\u2019s consider four groups.\u000ATabela 2: Number of survivors or non-survivors there are in each Age group after the infection. Cal- culated for covid_spring data.\u000A25 For a categorical random varia- ble with probability pc for class c entropy isPdefined as\u000AH=- cpclog2pc,\u000Awhile GinPi impurity is defined as G = 1- c p2c. Check that the\u000AGini impurity for the root node in our example is 0.0973.\u000ATabela 3: Let us consider three po- ssible splits of the variable Age and step by step we calculate the pro- babilities of each class, the purity of each node and the weighted pu- rity of the split. The best split is for age 70, although for both the younger and older age groups the purity is worse than for the other splits. The weights defining the si- ze of the nodes proved to be crucial in this example.\u000AAge group / Status\u000ASurvived Died\u000A630 31-50 2250 3716 6 17\u000A51-70 >70 Total 2760 729 9487 153 337 513\u000A We consider three splits for cutoffs of 30, 50, and 70. For each split, we calculate the probability of death and survival in that gro- up. We then calculate the purity25 of each of the resulting nodes. In the example below, the Gini value is used, but entropy or statistical tests are also commonly used. The final split purity is defined as the weighted node purity considering the number of observations at each node. The smaller the value, the better. From the options below, we get the best purity for a cutoff of 70 years.\u000APossible split 30 50 70 nodei 6>6>6>\u000A pi,D\u000Api,S\u000AG = 1 - p2\u000Ai i,D\u000Anode weight wi w6G6 + w>G>\u000A0.0027 0.9973 0.0053 0.2263\u000A0.066 0.0038\u000A0.934 0.9962 0.1228 0.00765 0.7737 0. 6008\u000A0.123 0.0198 0.877 0.9802 0.216 0.0388\u000A0.3992 0.8931\u000A0.316\u000A0.684 0.4324 0.1070\u000A- p2\u000A i,S\u000A 0.0962 0.0908\u000A0.0809\u000AThee second key parameter for training a tree is the choice of the stopping criterion. Each split increases the purity of subsequent nodes, so the deeper the tree is the higher purity of leaves. hus, large (deep) trees extract more relations from data, although some may be accidental (a phenomenon called over-fitting), which may result in poorer generalisation and worse results on new/validation data.\u000A","23\u000AModel Assembly\u000AR snippets\u000AThere are many libraries in R for training decision trees. The follo- wing snippets are based on the partykit26 library because it works for regression, classification and survival models has good statistical properties and has clear visualizations.\u000ATo train a tree, we use ctree function. The first argument is a for- mula describing which variable is the target and which are the expla- natory variables. 27The second argument indicates the training data. The control argument specifies additional parameters such as node splitting criteria, maximum tree depth or maximum node size.\u000Alibrary(\u0022partykit\u0022) _\u000Atree <- ctree(Death ~., covid spring,\u000A              control = ctree_control(alpha = 0.0001))\u000A# See Figure 8\u000Aplot(tree)\u000A26 Torsten Hothorn and Achim Ze- ileis. partykit: A modular toolkit for recursive partytioning in R. Jo- urnal of Machine Learning Research, 16:3905\u20133909, 2015\u000A27 In this package, statistical tests are used to evaluate separation for a split. In the example below, alpha = 0.0001 means that nodes will be split as long as the p-value is below 0.0001 for the \u00002 test for indepen- dence.\u000AFigure 8: The first split in the tree is for the Age variable. Patients are divided into younger than 67 (left) and older than 67 (right). In the same manner, one can read other splits. The criteria adopted resulted in a tree with seven leaves. The le- aves include information about the number of patients who reached that leaf and the proportion of each class.\u000A Cardiovascular.Diseases p < 0.001\u000ACancer p < 0.001\u000ANo\u000AYes\u000A5 Age\u000Ap < 0.001\u000A\u226462 >62\u000A1 Age\u000Ap < 0.001\u000A2\u226467 >679\u000A   Cancer p < 0.001\u000ANo Yes\u000A3 10\u000ACardiovascular.Diseases p < 0.001\u000ANoYes\u000ANode 6 (n = 352) Node 7 (n = 117) 0.8 0.8 0.8\u000A0.6 0.6 0.6\u000A0.4 0.4 0.4\u000A0.2 0.2 0.2 0000000\u000ANode 4 (n = 8096) 1111111\u000ANode 8 (n = 124)\u000ANode 11 (n = 618)\u000ANode 12 (n = 67)\u000ANode 13 (n = 626)\u000A0.8 0.8 0.6 0.6 0.4 0.4 0.2 0.2\u000A0.8 0.8 0.6 0.6 0.4 0.4 0.2 0.2\u000ANo\u000AYes\u000AThe explain function builds a uniform interface to query the mo- del. Note that the predict_function is different than for CDC mo- del, it is specific to party objects. Subsequent arguments indicate the test data for the explain count, model type and label.\u000Amodel_tree <- DALEX::explain(tree, predict_function = function(m, x)\u000A                    predict(m, x, type = \u0022prob\u0022)[,2],\u000A           data = covid_summer,\u000A           y = covid_summer$Death == \u0022Yes\u0022,\u000A           type = \u0022classification\u0022, label = \u0022Tree\u0022)\u000AOnce the explainer is prepared, we can check how good this mo- del is. It looks like it is better than the CDC model both on the training and validation data.\u000A(mp_tree <- model_performance(model_tree, cutoff = 0.1))\u000A# Measures for:  classification\u000A# recall     : 0.8626609\u000A# precision  : 0.1492205\u000AReceiver Operator Characteristic Model CDC Tree\u000A 1.00\u000A0.75\u000A0.50\u000A0.25\u000A0.00 0.00\u000A0.25 0.50 0.75 1.00 False positive rate\u000A          # f1\u000A# accuracy\u000A# auc\u000A# See Figure 9\u000Aplot(mp_tree, mp_cdc, geom=\u0022roc\u0022)\u000A: 0.2544304\u000A: 0.8822\u000A: 0.9136169\u000AFigure 9: ROC curves for the CDC and tree model. Tree model has on average better predictions.\u000ATrue positive rate\u000AYes No\u000AYes No\u000AYes No\u000AYes No\u000AYes No\u000AYes No\u000AYes No\u000A"," "," ","Model Assembly\u000A26\u000A28 Leo Breiman. Random forests. Machine Learning, 45(1):5\u201332, 2001a. ISSN 0885-6125\u000A29 The term bootstrap refers to the saying \u0022pull oneself up by one\u2019s bo- otstraps\u0022 which relates to one of the tales of Baron Munchausen. It means to solve an impossible pro- blem without outside help. In the original, the Baron pulled himself out of the swamp by his own ha- ir. In the case of random forests, we have no new data, yet by cre- ating bootstrap copies, we are able to control and reduce the variance of the predictive model.\u000AFigure 10: The key steps are: to ge- nerate a set of B bootstrap copies of the dataset, generated by sam- pling with replacement. Deep tre- es are trained for each copy. To in- crease the variability between tre- es, the procedure for split selec- tion is changed in a way, that only a random subset of m variables is considered for a single node. Du- ring the prediction, the results of the individual trees are aggregated. Boostrap samples have out-of-bag (OOB) subsets, i.e. observations the- re were not selected during sam- pling, on which the performance of the model can be evaluated. A de- tailed description of the random forest algorithm is available at https://tinyurl.com/RF2001.\u000AR snippets\u000APlant a forest\u000ADecision trees have many advantages, especially when it comes to interpretability and transparency. From a modelling perspective, de- ep trees have low bias but high variance (easily overfit to the data), while shallow trees have low variance but high bias (do not catch some relations). Can we improve both flexibility and stability?\u000AIn 2001, Leo Breiman proposed a new family of models, called ran- dom forests28, which aggregate decisions from an ensemble of deep trees trained on bootstrap samples of the data. Bootstrap29 is today a very widespread and powerful statistical procedure. It creates B copies of the data, called bootstrap samples, by sampling with re- placement. One tree is trained on each copy of the data. During the prediction phase, the results from particular trees are aggregated. See Figure 10 for more details. Such a procedure improves model generalization by reducing the variance of individual trees.\u000ATraining a random forest requires specifying a set of hyperpara- meters such as B - the number of trees, m - the size of the subset of variables from which to select split candidates for a single node, maximum tree depth, minimum node size, etc. We say more about the selection of hyperparameters in the next section, but fortunately the random forest algorithm is quite robust to the selection of hyper- parameters. Thanks to all these advantages, random forest is a very popular and efficient technique for predictive modelling.\u000ACreate B bootstrap samples of the data\u000AXy\u000ATrain deep trees. In each node consider only m variables as possible splits\u000ACombine individual predictions\u000Avote!\u000A  X*1\u000AX*2\u000AX*3\u000AX*4\u000Ay*1\u000Ay*2\u000Ay*3\u000Ay*4\u000A             30 Andy Liaw and Matthew Wie- ner. Classification and Regression by randomForest. R News, 2(3):18\u2013 22, 2002\u000A31 Marvin N. Wright and Andreas Ziegler. ranger: A fast implemen- tation of random forests for high dimensional data in C++ and R. Jo- urnal of Statistical Software, 77(1):1\u2013 17, 2017\u000AThe two most popular packages for training random forests in R are randomForest30 and ranger31. Both are easy to use, efficient and well parametrized. But here, we use mlr3 toolkit for model training. It adds an additional level of abstraction, a little more complex to use, but has additional features that we will be used in the next section devoted to hyperparameters.\u000A","27\u000ATraining a model with mlr332 is performed in three steps.\u000A1. Define the prediction task, an object that remembers on what data which variable shall be predicted\u000Alibrary(\u0022mlr3\u0022)\u000A(covid_task <- TaskClassif$new(id = \u0022covid_spring\u0022,\u000A                         _\u000A          backend = covid spring,\u000A          target = \u0022Death\u0022,  positive = \u0022Yes\u0022))\u000A  # <TaskClassif:covid_spring> (10000 x 8)\u000A  # * Target: Death\u000A  # * Properties: twoclass\u000A  # * Features (7):\u000AModel Assembly\u000A32 Michel Lang, Martin Binder, Ja- kob Richter, Patrick Schratz, Flo- rian Pfisterer, Stefan Coors, Qu- ay Au, Giuseppe Casalicchio, Lars Kotthoff, and Bernd Bischl. mlr3: A modern object-oriented machine learning framework in R. Journal of Open Source Software, 2019. doi: 10.21105/joss.01903\u000A# # #\u000A- fct (6): Cancer, Cardiovascular.Diseases, Diabetes, Gender, Kidney.Diseases, Neurological.Diseases\u000A- int (1): Age\u000A2. Select the family of models in which we want to look for a solu- tion. There is a lot of algorithms to choose from, see the documen- tation. Set \u0022classif.ranger\u0022 for the random forests models.\u000Alibrary(\u0022mlr3learners\u0022)\u000Alibrary(\u0022ranger\u0022)\u000Acovid_ranger <- lrn(\u0022classif.ranger\u0022, predict_type=\u0022prob\u0022,\u000Anum.trees=25)\u000A3. Train the model with the train() method. The mlr3 package is\u000Ausing R6 classes, so this method modifies the object in place. covid_ranger$train(covid_task)\u000AA trained model can be turned into a DALEX explainer. Note that the predict_function is again slightly different. DALEX would guess it based on the class of the model, but we point it out explicitly to make it easier to understand what is going on underneath.\u000Amodel_ranger <- explain(covid_ranger, predict_function = function(m,x)\u000A               predict(m, x, predict_type = \u0022prob\u0022)[,1],\u000A          data = covid_summer,\u000A          y = covid_summer$Death == \u0022Yes\u0022,\u000A          type = \u0022classification\u0022, label = \u0022Ranger\u0022)\u000AWe can now check how good this model is. As expected, a random forest model has better performance/AUC than a single tree.\u000A(mp_ranger <- model_performance(model_ranger))\u000A# Measures for:  classification\u000A# recall     : 0.04291845\u000A# precision  : 0.4347826\u000AReceiver Operator Characteristic\u000AModel Ranger\u000A0.00\u000A0.00 0.25 0.50\u000ACDC Tree\u000A1.00\u000A0.75\u000A0.50\u000A0.25\u000A            # f1\u000A# accuracy\u000A# auc\u000A: 0.078125\u000A: 0.9764\u000A: 0.9425837\u000A0.75 1.00 False positive rate\u000A # See Figure 11\u000Aplot(mp_ranger, mp_tree, mp_cdc, geom= \u0022roc\u0022)\u000AFigure 11: ROC curves for the CDC, tree and ranger model.\u000ATrue positive rate\u000A"," "," ","Model Assembly\u000A30\u000A33 Each of following steps can be im- plemented in many ways, so there is no single best way to tune mo- dels. We show an example frame- work for tabular data.\u000AFigure 12: The hyperparameter optimization scheme implemented in the mlr3tuning package. So- urce: https://mlr3book.mlr-org. com/tuning.html\u000AHyperparameter Optimisation\u000AMachine Learning algorithms typically have many hyperparameters that specify model training process. For some models families, li- ke Support Vector Machines (SVM) or Gradient Boosting Machines (GBM), the selection of such hyperparameters has a strong impact on the performance of the final model. The process of finding good hyperparameters, a process that is commonly called tuning.\u000AThe general optimization scheme33 is described in Figure 12. Diffe- rent model families have different sets of hyperparameters. We don\u2019t always want to optimize all of them simultaneously, so the first step is to define the hyperparameter search space. Once it is specified, then tuning is based on a looped two steps: (1) select a set of hyper- parameters and (2) evaluate how good is this set of hyperparame- ters. These steps are repeated as long as some stopping criterion is met, such as the maximum number of iterations, desired minimum model performance, or some increase in model performance.\u000A  Start\u000ASuggest Hyperparameters\u000A Search Space Tuner\u000ATerminator (when to finish\u000AEnd\u000A   Let\u2019s focus a little more attention on the process of evaluating sets of hyperprameters. One of the key principles of machine lear- ning is that the model should be verified on different data than that used for training. Even if we have separate data for training and final testing, we must not sneak a peek or use that test data when evaluating hyperparameters. We need to generate internal test data for the hyperparameter evaluation. This is often done using internal cross-validation. See an example on the next page.\u000AR snippets\u000AThe example below uses the mlr3 package. Other interesting solu- tions for hyperparameter optimization in R are h2o and tidymodels. First, we need to specify the space of hyperparameters to search.\u000ANot all hyperparameters are worth optimizing. Let\u2019s focus on four for the random forest algorithm.\u000Alibrary(\u0022mlr3tuning\u0022) library(\u0022paradox\u0022) search_space = ps(\u000Anum.trees = p_int(lower = 50, upper = 500), max.depth = p_int(lower = 1, upper = 10),\u000Aminprop = p_dbl(lower = 0.01, upper = 0.1), splitrule = p_fct(levels = c(\u0022gini\u0022, \u0022extratrees\u0022))\u000A)\u000AEvaulate Performance\u000A","31\u000AModel Assembly\u000AFor automatic hyperparameter search, it is necessary to specify: (1) a procedure to evaluate the performance of the proposed models, below it is the AUC determined by 5-fold cross-validation, (2) a se- arch strategy for the parameter space, below it is a random search, (3) a stopping criterion, below it is the number of 10 evaluations34.\u000A34 Of course, Bit having a High Per- formance Cluster (HPC) can check hundreds of thousands of hyperpa- rameter configurations as the who- le process is easily parallelized. Ho- wever, in this example we have fo- cused on reproducibility, so we pre- sent results for 10 configurations, making it easy for any reader to reproduce these results. Also, for this dataset, the default random fo- rest parameters give very good re- sults, so we wouldn\u2019t gain much with long tuning anyway.\u000A35 Note, that the AUC 0.9272979 presented below is not calculated on the covid_summer, but it is inter- nal evaluation of hyperparameters with the 5-fold CV procedure. AUC on the covid_summer is presented on the bottom of this page.\u000A36 Moreover, some algorithms, like random forests, are not very tuna- ble. Still, we had to try!\u000AReceiver Operator Characteristic\u000A     _\u000Atuned ranger =\u000A    learner\u000A    resampling = rsmp(\u0022cv\u0022, folds = 5),\u000A    measure    = msr(\u0022classif.auc\u0022),\u000A    search_space = search_space,\u000A                               _\u000A    terminator = trm(\u0022evals\u0022, n evals = 10),\u000A    tuner      = tnr(\u0022random_search\u0022) )\u000AOnce the optimization parameters have been defined, we can turn on their optimization with the train method, just as with any other predictive model in mlr3 framework35.\u000Atuned_ranger$train(covid_task)\u000Atuned_ranger$tuning_result\u000A#    num.trees max.depth    minprop splitrule\u000A# 1:       264         9 0.06907318      gini\u000A#    learner_param_vals  x_domain classif.auc\u000A# 1:          <list[4]> <list[4]>   0.9272979\u000AThere is, of course, no guarantee that the tuner will find better hyperparameters than the default ones36. But in this example, the tuned model is better than all other models that we have considered so far. Let\u2019s see how much. We need an DALEX wrapper.\u000Amodel_tuned <- explain(tuned_ranger, predict_function = function(m,x)\u000A        m$predict_newdata(newdata = x)$prob[,1],\u000A    data = covid_summer,\u000A    y = covid_summer$Death == \u0022Yes\u0022,\u000A    type = \u0022classification\u0022, label = \u0022AutoTune\u0022)\u000ASo we can calculate and compare model performance/AUC on validation data and then compare ROC curves for various models.\u000A(mp_tuned <- model_performance(model_tuned))\u000A# Measures for:  classification\u000A# recall     : 0.02575107\u000A# precision  : 0.4\u000AAutoTuner$new(\u000A= covid_ranger,\u000AModel Ranger 1.00\u000A0.75\u000A0.50\u000A0.25\u000A0.00\u000A0.00 0.25\u000AFigure 13: ROC curves for the CDC, tree, ranger model and auto tune ranger model.\u000AAutoTune CDC Tree\u000A          # f1\u000A# accuracy\u000A# auc\u000A# See Figure 13\u000Aplot(mp_tuned, mp_ranger, mp_tree, mp_cdc, geom = \u0022roc\u0022)\u000ANote on reproducibility: Take into account that the training is based on randomization, so you may get slightly different results on different computers or with different versions of packages. Even if you execute the same snippet twice, you may get slightly different results. Nevertheless, the general conclusions should be the same.\u000A: 0.0483871\u000A: 0.9764\u000A: 0.9447171\u000A0.50 0.75 1.00 False positive rate\u000ATrue positive rate\u000A"," "," ","Model Audit\u000A34\u000A  AUC\u000Af(x)\u000A  Var Imp\u000Aa\u000A Model\u000AInstance\u000AThe XAI pyramid describes rela- tions between explanatory model analysis techniques. The deeper, the more detailed view into the mo- del.\u000A37 The permutational variable im- portance is described in detail in Chapter 16 of Explanatory Mo- del Analysis https://ema.drwhy. ai/featureImportance.html\u000AFigure 14: Permutation of variables preserves the marginal distribution while breaking the dependence of that variable on the target.\u000AVariable-importance\u000AWhen we examine a high-dimensional model, one of the first qu- estions that come up are: which variables are important? which features or groups of features significantly affect the model\u2019s performance?\u000ASome models have built-in methods for the assessment of variable importance. For example, for linear models, one can use standardi- zed model coefficients or p-values. For the random forest, one can use out-of-bag classification error. For tree boosting models, one can use information gain statistics. Yet, the problem with such model- specific techniques is that they cannot be compared between mo- dels of different structures. For this and few other reasons, it is co- nvenient to use model agnostic techniques, such as permutational importance of variables37 .\u000AThe procedure is based on perturbations of a selected variable or group of variables. The intuition is that if a variable is important in a model, then after its random perturbation, the model predictions should be less accurate.\u000AThe permutation-based variable-importance of a variable i is the difference (or ratio) between the model performance for the origi- nal data and the model performance calculated on data with the permuted variable i. More formally\u000AVI(i) = L(f, Xperm(i), y) - L(f, X, y),\u000Awhere L(f, X, y) is the value of loss function or performance me- asure for the data X, true labels y and model f, while Xperm(i) is dataset x with i-th variable permuted.\u000ANote that the importance of the variables defined in such a way can be determined without re-training of the model.\u000AWhich performance measure should you choose? It\u2019s up to you. In the DALEX library, by default, RMSE is used for regression and 1-AUC for classification problems. But you can change the loss func- tion by specifying the loss_function argument.\u000AR snippets\u000AWe use the model_parts function from the DALEX package to calcu- late the importance of variables. The only required argument is the model to be analyzed. With additional arguments, one can also spe- cify how the importance of variables is to be calculated, whether as a difference, ratio or without normalization. The last line _baseline_ of the following listing corresponds to the difference in loss function of a model calculated on data in which all variables have been per- muted.\u000Ampart_ranger <- model_parts(model_ranger, type=\u0022difference\u0022) mpart_ranger\u000A # # 1 # 2\u000A             variable mean_dropout_loss  label\u000A         _full_model_      0.0000000000 Ranger\u000ANeurological.Diseases      0.0006254491 Ranger\u000A","35\u000AModel Audit\u000A#3 #4 #5 #6 #7 #8 #9\u000A                 Gender\u000A        Kidney.Diseases\u000A                 Cancer\u000A               Diabetes\u000ACardiovascular.Diseases\u000A                    Age\u000A             _baseline_\u000A0.0030246808 Ranger\u000A0.0048972639 Ranger\u000A0.0061278070 Ranger\u000A0.0076210243 Ranger\u000A0.0207565006 Ranger\u000A0.1580579207 Ranger\u000A0.4203818555 Ranger\u000AThis technique is handy when we want to compare the importan- ce of variables in different models. Let\u2019s see how it looks for our example. The generic plot function works for any number of mo- dels given as consecutive arguments.\u000Ampart_cdc    <- model_parts(model_cdc)\u000Ampart_tree   <- model_parts(model_tree)\u000Ampart_ranger <- model_parts(model_ranger)\u000Ampart_tuned  <- model_parts(model_tuned)\u000A# See Figure 15\u000Aplot(mpart_cdc, mpart_tree, mpart_ranger, mpart_tuned, show_boxplots = FALSE)\u000A Variable importance AutoTune\u000ACDC\u000ATree\u000A                      Age Cardiovascular.Diseases Cancer\u000AGender\u000AKidney.Diseases Diabetes Neurological.Diseases\u000ARanger\u000A0.1\u000AAge Cardiovascular.Diseases Cancer\u000AGender\u000AKidney.Diseases Diabetes Neurological.Diseases\u000A                        Age Cardiovascular.Diseases Cancer\u000AGender\u000AKidney.Diseases Diabetes Neurological.Diseases\u000AAge Cardiovascular.Diseases Cancer\u000AGender\u000AKidney.Diseases Diabetes Neurological.Diseases\u000A  0.2\u000A0.3\u000A0.4 0.5\u000AOne minus AUC loss after permutations\u000A0.1 0.2 0.3\u000A0.4 0\u000ALooking for more?\u000AThe same perturbation technique can be used to analyze the impor- tance of groups of variables. Just use the variable_groups argument. Grouping variables can be particularly useful if the number of varia- bles is large and groups of variables describe some common aspects. In our case we could group all diseases together.\u000AFor highly correlated variables, an interesting model exploration technique is triplot, which summarise correlations structure via dendrogram and also show the importance of groups of correlated variables. Still, variable importance analysis when variables are cor- related must be performed with care.\u000A.5\u000AFigure 15: The importance of va- riables can be compared between models, and it is usually a sour- ce of valuable information. In this plot, each bar starts at 1-AUC for the model on the original data and ends at 1-average AUC calculated on the data with the indicated va- riable permuted.\u000AFor the CDC model, the only important variable is Age. For the tree model, the three important va- riables are Age, Cancer, and Car- diovascular diseases, an observa- tion consistent with Figure 8. For the ranger model and the model after tuning of hyperparameters, more variables are taken into acco- unt. However, Age is indisputably the most important variable in all models.\u000A"," "," ","Model Audit\u000A38\u000A AUC\u000Af(x)\u000A PD / ALE\u000AModel\u000AInstance\u000ABoth methods are described in detail in Chapter 17 of Explanatory Model Analy- sis https://ema.drwhy.ai/ partialDependenceProfiles.html\u000AFigure 16: In the data set, the va- riable i is replaced by the value t, then average model response is cal- culated.\u000APartial Dependence and Accumulated Local Effects\u000AOnce we know which variables are important, it is usually intere- sting to determine the relationship between a particular variable and the model prediction. Popular techniques for this type of Explanato- ry Model Analysis are Partial Dependence (PD) and Accumulated Local Effects (ALE).\u000APD profiles were initially proposed in 2001 for gradient boosting models but can be used in a model agnostic fashion. This method is based on analysis of average model response after replacing the variable i with the value of t.\u000AMore formally, Partial Dependence profile for a variable i is a function of t defined as\u000APD(i, t) = E [f(x1, ..., xi-1, t, xi+1, ..., xp)] ,\u000Awhere the expected value is calculated over the data distribution.\u000AThe straightforward estimator is\u000A1 Xn\u000APcD(i, t) = n f(xj1, ..., xji-1, t, xji+1, ..., xjp).\u000Aj=1\u000AReplacing i-th variable by the value t can lead to very strange observations, especially when i-th variable is correlated with other variables, and we ignore the correlation structure. One solution to this are Accumulated Local Effects profiles which average over the conditional distribution.\u000AShowing a variable response profile carries a lot of information. However, keep in mind that in complex models, you should expect complex interactions. Thus, one global profile for a variable may be an oversimplification. An extension of PD profiles is to calculate them in subgroups defined by some other variables or based on segments of observations found from model responses. You will find some examples below.\u000AR snippets\u000AWe use the model_profile function from the DALEX package to cal- culate the variable profile. The only required argument is the model to be analyzed. It is a good idea to specify names of variables for profile estimation as a second argument, otherwise, profiles are cal- culated for all variables, which can take some time. One can also specify the exact grid of values for calculations of profiles.\u000AThe average is calculated for the distribution specified in the data argument in the explainer. Here we calculate the PD profiles for the Age variable for covid_summer data.\u000Amp_ranger <- model_profile(model_ranger, \u0022Age\u0022)\u000A# See Figure 17\u000Aplot(mp_ranger)\u000A  PD profile\u000AAge 0.3\u000A0.2\u000A0.1\u000A       0\u000AFigure 17: Partial dependence pro- file for Age variable.\u000A0 25 50 75 10\u000Aaverage prediction\u000A","39\u000AModel Audit\u000ASince we have four models it is worth comparing how they differ in terms of the model\u2019s response to the Age variable.\u000Amp_cdc    <- model_profile(model_cdc, \u0022Age\u0022)\u000Amp_tree   <- model_profile(model_tree, \u0022Age\u0022)\u000Amp_tuned  <- model_profile(model_tuned, \u0022Age\u0022)\u000A# See Figure 20\u000Aplot(model_cdc, model_tree, mp_ranger, model_tuned)\u000A Age 0.3\u000A0.2\u000A0.1\u000AAutoTune\u000ACDC Ranger Tree\u000A       0.0\u000A0 25 50 75 100\u000A  Grouped Partial Dependence profiles\u000ABy default, the average is calculated for all observations. But with the argument groups one can specify a grouping variable. PD profi- les are calculated independently for each level of this variable.\u000Amgroup_ranger <- model_profile(model_ranger, \u0022Age\u0022, groups = \u0022Diabetes\u0022)\u000A# See Figure 19\u000Aplot(mgroup_ranger)\u000AClustered Partial Dependence profiles\u000AIf the model is additive, then individual profiles (see the next Sec- tion related to Ceteris Paribus profiles) are parallel. But if the model has interactions, individual profiles may have different shapes for different values of variables in interaction. To see if there are such interactions we can cluster the individual profiles.\u000AIf we specify the argument k then the function model_profile per- forms a hierarchical clustering of the profiles, determine the group of k most different profiles and then calculate the Partial Dependen- ce for each of these groups separately.\u000A0\u000AFigure 19: Partial Dependence for Age in groups defined by Diabetes variable.\u000AFigure 18: Each colour indicates a different model. The CDC model has a shifted jump in risk of death. Models based on covid_spring da- ta are more likely to place the dra- matic increase in the risk around age 65. The tree model is too shal- low to capture the ever-increasing risk in the oldest group. Despite this, the models are quite consi- stent about the general shape of the relationship.\u000APD profiles for groups\u000A0.5 Age 0.4\u000A0.3\u000A0.2\u000A0.1 0.0\u000ARanger_No\u000ARanger_Yes\u000A             mclust_ranger <- model_profile(model_ranger, \u0022Age\u0022, k=3,center=TRUE)0\u000A# See Figure 20\u000Aplot(mclust_ranger)\u000AFigure 20: Partial Dependence for three segments.\u000A0\u000A25 50 75 10\u000A PD profiles for segments Age\u000A0.6 0.4 0.2 0.0\u000A       0 25 50 75 10\u000Aaverage prediction\u000Aaverage prediction\u000A"," "," ","Model Audit\u000A42\u000A AUC\u000Af(x)\u000A SHAP / BD\u000AModel\u000AInstance\u000AInstance level exploration\u000AFrom the model developer perspective, we are often interested in the global behaviour of a model, i.e. whether it has high performance or how it changes on average as s function of some feature. But the user perspective is different. In most cases, a user is interested in an individual prediction related to him or her. Often we hear about the\u000A\u201Eright to explanation\u201D, which means that for a model prediction, we should be able to find out which variables significantly influenced the model prediction. Especially for high-stake decisions, we should enrich model predictions with as much information as possible to support informed and responsible predictions.\u000AShapley values and the Break-down plots\u000AFor tabular data, one of the most commonly used techniques for local variable attribution is Shapley values. The key idea behind this method is to analyze the sequence of conditional expected values. This way, we can trace how the conditional mean moves from the average model response to the model prediction for observation of interest x\u21E4. Let\u2019s consider a sequence of expected values.\u000A\u03BC = E[f(X)],\u000A\u03BC x 1 = E [ f ( X ) | X 1 = x \u21E41 ] ,\u000A\u03BCx1,x2 = E [f(X)|X1 = x\u21E41, X2 = x\u21E42] ,\u000A...\u21E5 \u21E4 \u21E4 \u21E4\u21E4\u21E4\u000A\u03BCx1,x2,...,xp = E f(X)|X1 = x1, X2 = x2, ..., Xp = xp = f(x ).\u000ABy looking at consecutive differences \u03BCx1 - \u03BC, \u03BCx1,x2 - \u03BCx1 and so on, one can calculate the added effects of individual variables, see an example in Figure 21. Sound like a straightforward solution, ho- wever, there are two issues with this approach.\u000AOne is that it is not easy to estimate conditional expected value. In most implementations, it is assumed that features are independent, and then we can estimate \u03BCK as an average model response with va- riables in set K replaced by corresponding values from observation x\u21E4. So the crude estimate would be\u000A\u03BCbK =\u000A1 Xn ni=1\u000Af(xo1,xo2,...,xop), where\u000A\u0000 xo = x\u21E4, if j 2 K j j\u000Axoj =xij,ifj62K.\u000AThe second issue is that these effects may depend on the order of conditioning. How to solve this problem? The Shapley values method calculates attributions as an average of all (or at least a large number of random) orderings, while the Break-down method uses a single ordering determined with a greedy heuristic that prefers variables with the largest attribution on the beginning.\u000A"," 43\u000AModel Audit\u000AR snippets\u000Aall data\u000AAge = 76 Cardiovascular.Diseases = Yes Gender = Male Kidney.Diseases = No\u000ACancer = No\u000ADiabetes = No Neurological.Diseases = No\u000AConsecutive conditoning for Ranger\u000AFigure 21: The following rows show the conditional distributions (vio- plots) and the conditional expected value (red dot). The grey lines be- tween the rows show how the pre- dictions for each observation chan- ge after replacing the next variable with the value from observation x\u21E4 . Analyzing such a sequence of con- ditionings, we can read which va- riables significantly explain the dif- ferences between the mean model response (first row) and the obse- rved model response (last row).\u000A0.0 0.2\u000A0.4 0.6\u000ALet\u2019s define an observation for which we will examine the model more closely. Let it be a 76-year-old man with hypertension. We show local model analysis using the model_ranger as an example.\u000ASteve <- data.frame(Gender\u000A   Age\u000A   Cardiovascular.Diseases\u000A   Diabetes\u000A   Neurological.Diseases\u000A   Kidney.Diseases\u000A                           = factor(\u0022Male\u0022, c(\u0022Female\u0022, \u0022Male\u0022)),\u000A                           = 76,\u000A                           = factor(\u0022Yes\u0022, c(\u0022No\u0022, \u0022Yes\u0022)),\u000A                           = factor(\u0022No\u0022, c(\u0022No\u0022, \u0022Yes\u0022)),\u000A                           = factor(\u0022No\u0022, c(\u0022No\u0022, \u0022Yes\u0022)),\u000A                           = factor(\u0022No\u0022, c(\u0022No\u0022, \u0022Yes\u0022)),\u000A                           = factor(\u0022No\u0022, c(\u0022No\u0022, \u0022Yes\u0022)))\u000AThe predict_parts function for a specified model and a specified observation calculates local variable attributions. The optional argu- ment order forces to use a specified sequence of variables. If not specified, then a greedy heuristic is used to start conditioning with the most relevant variables. Results are presented in Figure 22.\u000A(bd_ranger <- predict_parts(model_ranger, Steve))\u000A   Cancer\u000Apredict(model_ranger, Steve)\u000A# 0.322\u000A#\u000A# Ranger: intercept\u000A# Ranger: Age = 76\u000A# Ranger: Cardiovascular.Diseases = Yes\u000A# Ranger: Gender = Male\u000A# Ranger: Kidney.Diseases = No\u000A# Ranger: Cancer = No\u000A# Ranger: Diabetes = No\u000A# Ranger: Neurological.Diseases = No\u000A# Ranger: prediction\u000Aplot(bd_ranger)\u000Acontribution\u000A    0.043\u000A    0.181\u000A    0.069\u000A    0.033\u000A   -0.004\u000A   -0.002\u000A    0.003\u000A    0.000\u000A    0.322\u000AThe alternative is to average over all (or at least many random) or- derings of variables. This is how the Shapley values are calculated. The show_boxplots argument highlights the stability of the estima- ted attributions between different orderings. See Figure 22.\u000Ashap_ranger <- predict_parts(model_ranger, Steve, type = \u0022shap\u0022) plot(shap_ranger, show_boxplots = TRUE)\u000A","Model Audit\u000A44\u000A Shapley values for Ranger\u000ABreak-down for Ranger\u000A                      Age = 76 Cardiovascular.Diseases = Yes Gender = Male Kidney.Diseases = No\u000ACancer = No\u000ADiabetes = No Neurological.Diseases = No\u000Aintercept\u000AAge = 76 Cardiovascular.Diseases = Yes Gender = Male Kidney.Diseases = No\u000ACancer = No\u000ADiabetes = No Neurological.Diseases = No prediction\u000A0.043\u000A0.1 prediction\u000A +0.181 +0.069\u000A 0.0 0.1 contribution\u000A0.2\u000A0.0\u000A0.2\u000A+0.033 -0.004 -0.002 +0.003 +0 0.322\u000A0.3 0.4\u000AFigure 22: Shapley values (left) and Break-down (right) illustrate the contributions of each variable to the final model response. Both attri- bution techniques ensure that the sum of the individual attributions adds up to the final model predic- tion.\u000A38 This option can identify pa- irwise interactions, see Chap- ter 7 in https://ema.drwhy.ai/ iBreakDown.html.\u000AThe Shapley values are additive. For models with interactions, it is often too much of a simplification. Other possible values of the type argument are shap, break_down, break_down_interactions38 or oscillations.\u000ANote that by default, functions such as model_parts, predict_parts, _\u000Amodel profiles do not calculate statistics on the entire data set (may be time consuming), but on n_samples of random cases, and the en- tire procedure is repeated B times to estimate the error bars.\u000ACeteris Paribus\u000ACeteris Paribus (CP) is a Latin phrase for \u0022other things being equal\u0022. It is also a very useful technique for the analysis of model behavio- ur for a single observation. CP profiles, sometimes called Individual Conditional Expectations (ICE), show how the model response wo- uld change for a selected observation if a value for one variable was changed while leaving the other variables unchanged.\u000AWhile local variable attribution is a convenient technique for an- swering the question of which variables affect the prediction, the local profile analysis is a good technique for answering the question of how the model response depends on a particular variable. Or answering the question of what if...\u000AR snippets\u000AThe predict_profiles() function calculates Ceteris Paribus profiles for a selected model and selected observations. By default, it calcu- lates profiles for all variables, but one can limit this list with the variables vector of variables.\u000Acp_ranger <- predict_profile(model_ranger, Steve) cp_ranger\u000A# Top profiles :\u000A AUC\u000Af(x)\u000A CP / ICE\u000AModel\u000AInstance\u000A#\u000A# 1\u000A# 1.1\u000A# 11\u000A# 1.110   Male  0.99\u000AGender   Age Cardiovascular.Diseases Diabetes\u000AFemale 76.00\u000A  Male 76.00\u000A  Male  0.00\u000AYes       No\u000AYes       No\u000AYes       No\u000AYes       No\u000AThe calculated profiles can be drawn with the generic plot func- tion. As with other explanations in the DALEX library, multiple mo- dels can be plotted on a single graph. Although for technical reasons,\u000A","45\u000AModel Audit\u000Aquantitative and qualitative variables cannot be shown in a single chart. So if you want to show the importance of quality variables, you need to plot them separately.\u000AFigure 23 shows an example of a CP profile for continuous varia- ble Age and categorical variable Cardiovascular.Diseases.\u000A# See Figure 23\u000Aplot(cp_ranger, variables = \u0022Age\u0022)\u000Aplot(cp_ranger, variables = \u0022Cardiovascular.Diseases\u0022,\u000A        categorical_type = \u0022lines\u0022)\u000A Ceteris paribus for Ranger Age\u000A0.4\u000A0.2\u000A0.0\u000A0 25 50 75 100\u000ACeteris paribus for Ranger Cardiovascular.Diseases\u000A0.4\u000A0.2\u000A0.0\u000A               No Yes\u000A The plot function can combine multiple models, making it easier to see similarities and differences.\u000Acp_cdc <- predict_profile(model_cdc, Steve) cp_tree<-predict_profile(model_tree,Steve)0\u000Acp_tune <- predict_profile(model_tuned, Steve)\u000A# See Figure 24 Figure 24: CP profiles for Steve, co- plot(cp_cdc, cp_tree, cp_ranger, cp_tune, variables = \u0022Age\u0022) lors code four considered models.\u000ACP profiles are also useful for finding the importance of variables in a model. The more the profiles fluctuate, the more influential is the variable. Such a measure of importance is implemented in the predict_parts function under option type = \u0022oscillations\u002239.\u000A__\u000Apredict parts(model ranger, Steve, type = \u0022oscillations\u0022)\u000A                                1   0.22872998\u000A                                1   0.16371903\u000A                                1   0.09641507\u000A                                1   0.05052652\u000A                                1   0.03984208\u000A                                1   0.03308303\u000A                                1   0.03164090\u000A39 The size of the oscillation can be measured in many ways, by de- fault, it is an area between the CP profile and a horizontal line at the level of the model prediction.\u000A_vname_ _ids_ oscillations\u000AFigure 23: The dot shows the ob- servation under analysis. CP pro- file shows how the model predic- tion change for changes in the se- lected variable. On the left is the CP profile for the continuous varia- ble, on the right for the categorical variable. For a categorical variable, you can specify how the CP profi- les should be drawn by specifying the categorical_type argument.\u000A      #\u000A# 2\u000A# 6\u000A# 7\u000A# 4\u000A# 3 Cardiovascular.Diseases\u000A# 1                  Gender\u000A# 5   Neurological.Diseases\u000A            Age\u000AKidney.Diseases\u000A         Cancer\u000A       Diabetes\u000ACeteris paribus for Steve Age\u000A0.4\u000A0.2\u000A0.0\u000A0 25 50 75 10\u000A prediction\u000Aprediction\u000Aprediction\u000A"," "," ","Model Deployment\u000A48\u000A40 Hubert Baniecki and Przemyslaw Biecek. The Grammar of Interac- tive Explanatory Model Analysis. Arxiv, 2020. URL https://arxiv. org/abs/2005.00497\u000AFigure 25: modelStudio is an appli- cation that facilitates model explo- ration using a serverless site based on javascript. The user can confi- gure the content of each panel to look at the model from different perspectives.\u000AModel Deployment\u000AWe have made the model built for Covid data, along with the expla- nations described in this book, available at https://crs19.pl/ we- bpage. After two months, tens of thousands of people used it. What is important, with proper tools, the deployment of such a model is not difficult.\u000ATo obtain a safe and effective model, it is necessary to perform a detailed explanatory model analysis. Although we often don\u2019t have much time for it. That is why tools that facilitate fast and automated model exploration are so useful.\u000AOne of such tools is modelStudio40. It is a package that transforms an explainer into html page with javascript based interaction. Such html page is easy to save on a disk or share by email. The webpa- ge has various explanations pre-calculated, so its generation may be time-consuming, but the model exploration is very fast, and the feedback loop is tight.\u000AGenerating a modelStudio for an explainer is trivially easy.\u000Alibrary(\u0022modelStudio\u0022)\u000Ams <- modelStudio(model_ranger) # See Figure 25\u000Ams\u000AAn example dashboard built for a model for dozens of variables and several thousand rows on football player worth prediction ba- sed on the FIFA dataset is available at\u000Ahttps://pbiecek.github.io/explainFIFA20/.\u000A ","49\u000AModel Deployment\u000AIf we want to automate the comparison of several models, Arena is a very convenient tool for such exploration. It can work in two modes: live (with the server which adds the necessary statistics on the fly) and pre-calculated statistics. In the case of many models and large datasets, the live mode is much more convenient.\u000AThe dashboard is created with the create_arena function. Then with push_model and push_observations, one can add more models and more observations for model exploration. The resulting object can be turned into the live web application with the run_server function.\u000AThe snippet below turns four covid models into a dashboard.\u000Alibrary(\u0022arenar\u0022) library(\u0022dplyr\u0022)\u000Acovid_ar <- create_arena(live = TRUE) %>% push_model(model_cdc) %>% push_model(model_tree) %>% push_model(model_ranger) %>% push_model(model_tuned) %>% push_observations(Steve)\u000A# See Figure 26\u000Arun_server(covid_ar)\u000AAn example dashboard built for a model for dozens of variables and several thousand rows on football player worth prediction ba- sed on the FIFA dataset is available on page https://arena.drwhy. ai/?demo=1.\u000AFigure 26: arenar is a web applica- tion that facilitates exploration of multiple models.\u000A "," "," ","Epilogue\u000A52\u000AAbout the authors\u000APrzemys\u0142aw Biecek - graduated in mathematical statistics and so- ftware engineering at the Wroc\u0142aw University of Technology. Cur- rently, he conducts research on responsible artificial intelligence and teaches students at the Warsaw University of Technology and the University of Warsaw. He founded a group of data analysis enthu- siasts MI2 that develops methods and tools for responsible machine learning. In his free time, together with Beta and Bit, he travels the world and seeks adventure.\u000AAnna Kozak - graduated in mathematical statistics and data analy- sis at the Warsaw University of Technology. She currently teaches classes in data visualization and exploration techniques and con- ducts research in the area of responsible machine learning with the MI2 group. In free time she organizes workshops in the Data Science area or reads detective stories.\u000AAleksander Zawada - Cartoonist, illustrator, graphic designer, al- though he trained as an architect at the Warsaw University of Tech- nology. Professionally engaged in all forms of graphic creation. He runs the foundation cyberetyka.pl. Besides, he loves board games, role-playing and strategy games, sports, and emotions.\u000AAbout the book\u000ATwo students from the Warsaw University of Technology went he- ad to head in a competition announced by NASA. Mietek Bekker won, and it was his Lunar Roving Vehicle in the Apollo spacecraft that flew to the moon. How are such students born? Through con- tact with exceptional tutors. And what characterizes the latter? An original and inspiring delivery.\u000AAn exceptional textbook by professor Przemys\u0142aw Biecek is now in your hands.\u000AMarek Sta\u02DBczek Coach, storyteller, author of books\u000A   "];